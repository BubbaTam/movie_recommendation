{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Questions\n",
    "    - ways to recommend\n",
    "        - how to prioritise features\n",
    "        - what about films that don't have many tags\n",
    "        - when looking at the recommendation for 'Shanghai calling', the closest recommended film in that iteration was 'Twister'; the issue is that the films had no similar tags\n",
    "        - using **soft cosine similarity**: After checking CountVectorizer.get_feature_names() there are a lot of similarly semantic names.\n",
    "        - Using a Harry Potter film, the recommended films highly recommended films are also Harry potter films. What way/s to reduce alternative films not in the same franchise.\n",
    "    - code related\n",
    "        - what is the difference between CountVectorizer and tfidVectorizer\n",
    "\n",
    "- Process\n",
    "    - take the basic_data_1 and put it into an appropriate format\n",
    "    - Recommendation process\n",
    "        - this iteration will use cosine similarity\n",
    "            - How:\n",
    "                - make a feature of each tag\n",
    "                - implementing the sklearn's class 'sklearn.metrics.pairwise import cosine_similarity'. This looks at the angle for each feature in the vector space. The smaller the angle, the higher the similarity score. For this use case, I will choose between the top 5 or 10 films, in terms of similarity score. A problem noticed is for films with too low similarity score; is there a point in recommending them? Using the basic_data_1 dataset,\n",
    "\n",
    "- bugs:\n",
    "    - ***[Fixed]*** when attempting to recommend Shanghai Calling the following error came up 'IndexError: index 4793 is out of bounds for axis 0 with size 4767'\n",
    "        - when removing null values --> ***[solution]*** reset the index during data clean\n",
    "    - ***[Fixed]*** vectorising the tags\n",
    "        - currently: when tokenising the tags, it tokens the words instead of the full phrase. This came up when I looked at recommendations for 'Shanghai Calling' and Harry Potter films came up. This is because they both have cast member that has the first name Daniel (Daniel Henney and Daniel Radcliffe).\n",
    "            - possible solutions \n",
    "                - find a way to cater for the each list entity\n",
    "                - ***[current solution]*** combine the phrases as one\n",
    "    - [To do] the tags encoding issues\n",
    "        - currently: 'LÃ©a Seydoux'\n",
    "- learnings\n",
    "    - dealing with words in foreign languages\n",
    "    - speech marks with data frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def working_directory():\n",
    "    \"\"\"fixed the issue\n",
    "\n",
    "    Returns:\n",
    "        str: the location of the ideal directory\n",
    "    \"\"\"\n",
    "    return os.getcwd().replace(\"\\\\notebooks\",\"\")\n",
    "os.chdir(working_directory())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import DATA_1\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import joblib\n",
    "\n",
    "from config import DATA_1, SAVED_PARAMETER_FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(dataframe):\n",
    "    df = dataframe.dropna(axis=0)\n",
    "    df = df.drop(columns=\"\u0004XuÎ\u0015\")\n",
    "    df = df.reset_index(drop=True)\n",
    "    df.id = df.id.apply(lambda x: str(int(x)))\n",
    "    # fix for word vectorisation\n",
    "    df.tags = df.tags.apply(lambda x: x.strip(\"'\").replace(\"'\",\"\").replace(\"[\",\"\").replace(\"]\",\"\").replace(\" \",\"\").lower().split(\",\"))\n",
    "    df.tags = df.tags.apply(lambda x: str(x))\n",
    "    return df\n",
    "def recommend_movie(movie):\n",
    "    movie_index = df[df['title_x'] == movie].index[0]\n",
    "    similarity = sorted(list(enumerate(cs[movie_index])),reverse=True,key = lambda x: x[1])\n",
    "    for i in similarity[1:10]:\n",
    "        print(f\"{df.iloc[i[0]].title_x} at {i[1]:.0%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_1, encoding= 'unicode_escape',encoding_errors=\"strict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = clean(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vectorising the tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = cv.fit_transform(df.tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['saved_entities\\\\parameters\\\\cosine_similarity_array.pkl']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs = cosine_similarity(values.toarray())\n",
    "joblib.dump(cs,os.path.join(SAVED_PARAMETER_FOLDER,\"cosine_similarity_array.pkl\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Star Trek Into Darkness at 30%\n",
      "Jupiter Ascending at 25%\n",
      "Aliens at 24%\n",
      "Ender's Game at 21%\n",
      "Predators at 21%\n",
      "The Lovers at 20%\n",
      "Predator at 20%\n",
      "John Carter at 20%\n",
      "Jimmy Neutron: Boy Genius at 20%\n"
     ]
    }
   ],
   "source": [
    "#before\n",
    "recommend_movie('Avatar')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "04ac14935f6ed29b3349ee8f41114d2dfa2ba78ce87cf701ad9b7ca15955b787"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
